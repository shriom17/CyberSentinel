from flask import Blueprint, request, jsonify
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
import joblib
import os
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import our ML models (simplified without TensorFlow for now)
try:
    from ml.crime_prediction_models import crime_models
    ML_MODELS_AVAILABLE = True
except ImportError:
    print("âš ï¸ ML models not available - using fallback predictions")
    ML_MODELS_AVAILABLE = False

analytics_bp = Blueprint('analytics', __name__)
import os
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import our ML models (simplified without TensorFlow for now)
try:
    from ml.crime_prediction_models import crime_models
    ML_MODELS_AVAILABLE = True
except ImportError:
    print("âš ï¸ ML models not available - using fallback predictions")
    ML_MODELS_AVAILABLE = False

analytics_bp = Blueprint('analytics', __name__)

class EnhancedPredictiveEngine:
    def __init__(self):
        # Initialize ML models
        if ML_MODELS_AVAILABLE:
            self.ml_models = crime_models
        else:
            self.ml_models = None
        
        # For now, disable deep learning models to avoid TensorFlow issues
        self.dl_models = None
        
        # Load pre-trained models or train new ones
        self.initialize_models()
    
    def initialize_models(self):
        """Initialize and load ML models"""
        try:
            # For now, just initialize without training heavy models
            # This prevents startup delays
            print("ðŸ¤– AI/ML Engine initialized (models will be loaded on demand)")
            
            # Set models as not loaded initially
            if self.ml_models:
                self.ml_models.models_loaded = False
            
        except Exception as e:
            print(f"âš ï¸  Error initializing models: {e}")
            # Continue without models for now
    
    def train_models_with_synthetic_data(self):
        """Train models with synthetic data for demonstration"""
        print("ðŸŽ¯ Training AI/ML models with synthetic data...")
        
        # Generate synthetic training data
        synthetic_data = self.generate_comprehensive_training_data()
        
        # Train traditional ML models
        self.ml_models.train_models(synthetic_data)
        
        # Train deep learning models
        self.dl_models.train_lstm_model(synthetic_data)
        self.dl_models.train_autoencoder(synthetic_data)
        
        print("âœ… All AI/ML models trained successfully")
    
    def generate_comprehensive_training_data(self, n_samples=5000):
        """Generate comprehensive synthetic training data"""
        np.random.seed(42)
        
        data = []
        base_date = datetime.now() - timedelta(days=365)
        
        for i in range(n_samples):
            # Create temporal patterns
            days_offset = np.random.randint(0, 365)
            hour = np.random.choice(range(24), p=self.get_hourly_probabilities())
            
            timestamp = base_date + timedelta(days=days_offset, hours=hour)
            
            # Create geographical clusters (Delhi NCR)
            cluster = np.random.choice(['Central Delhi', 'Gurgaon', 'Noida', 'Faridabad', 'Ghaziabad'])
            lat, lon = self.get_cluster_coordinates(cluster)
            
            # Create realistic amount patterns
            amount = self.generate_realistic_amount()
            
            # Create incident counts with patterns
            base_incidents = np.random.poisson(3)
            if hour in [14, 15, 16, 20, 21]:  # Peak hours
                base_incidents += np.random.poisson(2)
            if timestamp.weekday() in [4, 5, 6]:  # Weekend
                base_incidents += np.random.poisson(1)
            
            record = {
                'timestamp': timestamp,
                'latitude': lat + np.random.normal(0, 0.01),
                'longitude': lon + np.random.normal(0, 0.01),
                'amount_involved': amount,
                'complaint_category': np.random.choice([
                    'UPI Fraud', 'ATM Fraud', 'Net Banking', 'Mobile Banking', 
                    'Credit Card Fraud', 'Investment Fraud'
                ]),
                'incident_count': base_incidents,
                'risk_level': self.determine_risk_level(base_incidents, amount),
                'hour': hour,
                'day_of_week': timestamp.weekday()
            }
            data.append(record)
        
        return data
    
    def predict_hotspots_ai(self, data):
        """AI-powered hotspot prediction"""
        try:
            # Check if models are loaded, if not use fallback
            if not self.ml_models.models_loaded:
                print("ðŸ”„ Using fallback prediction (models not trained yet)")
                return self.predict_hotspots_fallback(data)
            
            # Use ML models for prediction
            predictions = self.ml_models.predict_hotspots(data.get('locations', []))
            
            # Enhance with deep learning anomaly detection if available
            if self.dl_models.models_loaded and len(predictions) > 0:
                anomaly_data = [{'latitude': p['location'].get('lat', 28.6139), 
                               'longitude': p['location'].get('lng', 77.2090),
                               'hour': datetime.now().hour,
                               'day_of_week': datetime.now().weekday(),
                               'amount_involved': 50000} for p in predictions]
                
                anomalies = self.dl_models.detect_anomalies(anomaly_data)
                
                # Mark high-anomaly locations as higher risk
                for i, pred in enumerate(predictions):
                    for anomaly in anomalies.get('anomalies', []):
                        if anomaly['index'] == i:
                            pred['anomaly_score'] = anomaly['anomaly_score']
                            pred['risk_score'] = min(1.0, pred['risk_score'] * 1.2)
            
            return predictions
            
        except Exception as e:
            print(f"Error in AI prediction: {e}")
            # Fallback to mock data
            return self.predict_hotspots_fallback(data)
    
    def predict_future_trends_ai(self, historical_data, hours_ahead=24):
        """AI-powered future trend prediction"""
        try:
            # Use LSTM for time series forecasting
            future_predictions = self.dl_models.predict_future_incidents(
                historical_data, hours_ahead
            )
            
            return future_predictions
            
        except Exception as e:
            print(f"Error in trend prediction: {e}")
            return {'error': str(e)}
    
    def analyze_patterns_ai(self, complaints_data):
        """AI-powered pattern analysis"""
        try:
            # Use ML clustering and pattern detection
            patterns = self.ml_models.detect_patterns(complaints_data)
            
            # Enhance with deep learning anomaly detection
            anomalies = self.dl_models.detect_anomalies(complaints_data)
            patterns['anomalies'] = anomalies
            
            return patterns
            
        except Exception as e:
            print(f"Error in pattern analysis: {e}")
            return self.get_fallback_patterns()
    
    def real_time_risk_assessment_ai(self, incident_data):
        """AI-powered real-time risk assessment"""
        try:
            # Get ML-based risk assessment
            assessment = self.ml_models.real_time_risk_assessment(incident_data)
            
            # Enhance with anomaly detection
            anomaly_result = self.dl_models.detect_anomalies([incident_data])
            
            if anomaly_result.get('anomalies_detected', 0) > 0:
                assessment['anomaly_detected'] = True
                assessment['anomaly_score'] = anomaly_result['anomalies'][0]['anomaly_score']
                assessment['risk_probability'] = min(1.0, assessment['risk_probability'] * 1.3)
            
            return assessment
            
        except Exception as e:
            print(f"Error in risk assessment: {e}")
            return {'error': str(e)}
    
    # Helper methods for synthetic data generation
    def get_hourly_probabilities(self):
        """Get realistic hourly probabilities for incidents"""
        # Higher probability during business hours and evening
        probs = np.array([0.02, 0.01, 0.01, 0.01, 0.02, 0.03, 0.04, 0.05,
                         0.06, 0.07, 0.08, 0.09, 0.08, 0.07, 0.09, 0.08,
                         0.07, 0.06, 0.05, 0.06, 0.07, 0.05, 0.04, 0.03])
        return probs / probs.sum()
    
    def get_cluster_coordinates(self, cluster):
        """Get coordinates for geographical clusters"""
        coordinates = {
            'Central Delhi': (28.6139, 77.2090),
            'Gurgaon': (28.4595, 77.0266),
            'Noida': (28.5355, 77.3910),
            'Faridabad': (28.4089, 77.3178),
            'Ghaziabad': (28.6692, 77.4538)
        }
        return coordinates.get(cluster, (28.6139, 77.2090))
    
    def generate_realistic_amount(self):
        """Generate realistic fraud amounts"""
        # Most frauds are small amounts, few are large
        if np.random.random() < 0.7:
            return np.random.exponential(5000)
        elif np.random.random() < 0.9:
            return np.random.exponential(25000)
        else:
            return np.random.exponential(100000)
    
    def determine_risk_level(self, incidents, amount):
        """Determine risk level based on incidents and amount"""
        risk_score = incidents * 0.3 + (amount / 100000) * 0.7
        
        if risk_score > 0.7:
            return 'high'
        elif risk_score > 0.4:
            return 'medium'
        else:
            return 'low'
    
    def predict_hotspots_fallback(self, data):
        """Fallback prediction method"""
        predictions = []
        
        for location in data.get('locations', []):
            risk_score = np.random.uniform(0.3, 0.95)
            predictions.append({
                'location': location,
                'risk_score': risk_score,
                'predicted_withdrawals': int(np.random.uniform(5, 50)),
                'confidence': np.random.uniform(0.7, 0.95),
                'factors': ['High complaint density', 'Weekend pattern', 'ATM proximity']
            })
        
        return sorted(predictions, key=lambda x: x['risk_score'], reverse=True)
    
    def get_fallback_patterns(self):
        """Fallback pattern analysis"""
        return {
            'temporal_patterns': {
                'peak_hours': [14, 15, 16, 20, 21],
                'peak_days': ['Friday', 'Saturday', 'Sunday'],
                'seasonal_trends': 'Increasing during festival seasons'
            },
            'geographical_patterns': {
                'high_risk_areas': ['Central Delhi', 'Gurgaon', 'Noida'],
                'emerging_hotspots': ['Faridabad', 'Ghaziabad']
            },
            'modus_operandi': {
                'top_methods': ['Phishing', 'OTP fraud', 'UPI scams'],
                'trending_methods': ['Fake investment apps', 'Romance scams']
            }
        }

# Initialize the enhanced engine
engine = EnhancedPredictiveEngine()

@analytics_bp.route('/predict', methods=['POST'])
def predict_hotspots():
    """AI-powered hotspot prediction"""
    try:
        data = request.get_json()
        predictions = engine.predict_hotspots_ai(data)
        
        return jsonify({
            'success': True,
            'predictions': predictions,
            'model_type': 'AI/ML Enhanced',
            'generated_at': datetime.now().isoformat()
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500

@analytics_bp.route('/patterns', methods=['GET'])
def analyze_patterns():
    """AI-powered pattern analysis"""
    try:
        # In production, fetch actual data from database
        complaints_data = engine.generate_comprehensive_training_data(100)
        patterns = engine.analyze_patterns_ai(complaints_data)
        
        return jsonify({
            'success': True,
            'patterns': patterns,
            'model_type': 'AI/ML Enhanced',
            'analyzed_at': datetime.now().isoformat()
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500

@analytics_bp.route('/predict-future', methods=['POST'])
def predict_future_trends():
    """AI-powered future trend prediction using LSTM"""
    try:
        data = request.get_json()
        hours_ahead = data.get('hours_ahead', 24)
        
        # Generate historical data for prediction
        historical_data = engine.generate_comprehensive_training_data(1000)
        
        future_predictions = engine.predict_future_trends_ai(historical_data, hours_ahead)
        
        return jsonify({
            'success': True,
            'future_predictions': future_predictions,
            'model_type': 'LSTM Deep Learning',
            'prediction_horizon_hours': hours_ahead
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500

@analytics_bp.route('/anomaly-detection', methods=['POST'])
def detect_anomalies():
    """Real-time anomaly detection using autoencoders"""
    try:
        data = request.get_json()
        current_incidents = data.get('incidents', [])
        
        anomaly_results = engine.dl_models.detect_anomalies(current_incidents)
        
        return jsonify({
            'success': True,
            'anomaly_results': anomaly_results,
            'model_type': 'Autoencoder Deep Learning'
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500

@analytics_bp.route('/risk-assessment', methods=['POST'])
def assess_risk():
    """AI-enhanced real-time risk assessment"""
    try:
        data = request.get_json()
        incident_data = {
            'latitude': data.get('latitude', 28.6139),
            'longitude': data.get('longitude', 77.2090),
            'amount_involved': data.get('amount', 50000),
            'hour': datetime.now().hour,
            'day_of_week': datetime.now().weekday(),
            'complaint_category': data.get('category', 'UPI Fraud')
        }
        
        assessment = engine.real_time_risk_assessment_ai(incident_data)
        
        return jsonify({
            'success': True,
            'risk_assessment': assessment,
            'model_type': 'AI/ML Enhanced'
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500

@analytics_bp.route('/model-status', methods=['GET'])
def get_model_status():
    """Get AI/ML model status and performance metrics"""
    try:
        status = {
            'traditional_ml_models': {
                'loaded': engine.ml_models.models_loaded,
                'models': ['RandomForestRegressor', 'GradientBoostingClassifier', 'DBSCAN'],
                'last_trained': datetime.now().isoformat()
            },
            'deep_learning_models': {
                'loaded': engine.dl_models.models_loaded,
                'models': ['LSTM', 'Autoencoder', 'CNN'],
                'frameworks': ['TensorFlow', 'Keras']
            },
            'capabilities': [
                'Hotspot Prediction',
                'Pattern Recognition',
                'Anomaly Detection', 
                'Time Series Forecasting',
                'Real-time Risk Assessment'
            ]
        }
        
        return jsonify({
            'success': True,
            'model_status': status
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500

@analytics_bp.route('/retrain-models', methods=['POST'])
def retrain_models():
    """Retrain AI/ML models with new data"""
    try:
        data = request.get_json()
        use_real_data = data.get('use_real_data', False)
        
        if use_real_data:
            # In production, fetch real data from database
            training_data = []  # Fetch from database
        else:
            # Use synthetic data for demonstration
            training_data = engine.generate_comprehensive_training_data(5000)
        
        # Retrain models
        engine.train_models_with_synthetic_data()
        
        return jsonify({
            'success': True,
            'message': 'AI/ML models retrained successfully',
            'training_samples': len(training_data),
            'retrained_at': datetime.now().isoformat()
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500